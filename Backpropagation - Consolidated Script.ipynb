{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/neural_network-1.png\">\n",
    "<img src=\"images/neural_network-2.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-bee55ae43449>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-bee55ae43449>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    Implementation of a A Step by Step Backpropagation Example by Matt Mazur\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "######\n",
    "# Implementation of a A Step by Step Backpropagation Example by Matt Mazur\n",
    "# https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "######\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "def initialize():\n",
    "    X = np.array([[0.05, 0.10]])      # Inputs\n",
    "    W1 = np.array([[0.15,0.20], [0.25,0.30]])      # Weights to calculate outputs for hidden layer 1\n",
    "    b1 = 0.35      # Bias for hidden layer 1\n",
    "    W2 = np.array([[0.40,0.45], [0.50,0.55]])     # Weights to calculate outputs for output layer\n",
    "    b2 = 0.60      # Bias for output layer\n",
    "    Y = np.array([[0.01, 0.99]])      # Desired output\n",
    "    learning_rate = 0.5\n",
    "    no_of_iter = int(10000)\n",
    "    return (X, W1, b1, W2, b2, Y, learning_rate, no_of_iter)\n",
    "    \n",
    "def forward_pass (X, W1, b1, W2, b2, Y):\n",
    "    ### Forward pass: Calculate hidden layer 1 (there is only 1 hidden layer in this example)\n",
    "    Z1 = np.dot(X,W1.T) + b1      # WtX + b\n",
    "    A1 = 1/(1 + np.exp(-Z1))       # Sigmoid(z) = 1 / (1 + e^(-z))\n",
    "    ### Forward pass: Calculate output layer\n",
    "    Z2 = np.dot(A1,W2.T) + b2      # WtX + b\n",
    "    A2 = 1/(1 + np.exp(-Z2))       # Sigmoid(z) = 1 / (1 + e^(-z))\n",
    "    ### Calculate error/cost function\n",
    "    E = np.sum(1/2*np.square(Y - A2))      # squared error function\n",
    "    return (A1, A2, E)\n",
    "\n",
    "def back_propagation(X, W1, W2, Y, A1, A2):\n",
    "    ### Back propogation\n",
    "    ### Adjust W2\n",
    "    dEdA2 = A2 - Y\n",
    "    dA2dZ2 = np.multiply (A2,1-A2)\n",
    "    dZ2dW2 = A1\n",
    "    dEdW2 = dEdA2 * dA2dZ2 * dZ2dW2\n",
    "    W2_adj = W2 - 0.5*dEdW2.T\n",
    "    W2 = W2_adj\n",
    "    ### Adjust W1\n",
    "    dZ2dA1 = W2.T\n",
    "    dA1dZ1 = np.multiply(A1,1-A1)\n",
    "    dZ1dW1 = X\n",
    "    dEdW1 = dEdA2 * dA2dZ2 * dZ2dA1 * dA1dZ1 * dZ1dW1\n",
    "    W1_adj = W1 - 0.5*dEdW1.T\n",
    "    W1 = W1_adj\n",
    "    return (W1, W2)\n",
    "\n",
    "def main():\n",
    "    (X, W1, b1, W2, b2, Y, learning_rate, no_of_iter) = initialize()\n",
    "    for iter in range (0,no_of_iter):\n",
    "        (A1, A2, E) = forward_pass(X, W1, b1, W2, b2, Y)\n",
    "        (W1, W2) = back_propagation(X, W1, W2, Y, A1, A2)\n",
    "    print ('W1 = {} \\n\\n W2 = {} \\n\\n Output = {} \\n Desired output = {} \\n Error = {}'.format(W1, W2, A2, Y, E))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
